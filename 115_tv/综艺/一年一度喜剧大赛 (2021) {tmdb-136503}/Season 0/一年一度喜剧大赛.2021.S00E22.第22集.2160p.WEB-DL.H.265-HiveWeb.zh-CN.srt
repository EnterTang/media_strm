1
00:00:06,970 --> 00:00:11,890
So I'm going to talk to you about Hive,
which is a database that runs on top

2
00:00:11,890 --> 00:00:15,335
of HDFS and
uses MapReduce to do its computation.

3
00:00:16,925 --> 00:00:19,375
The whole idea was
started that Facebook and

4
00:00:19,375 --> 00:00:23,215
it was to look at data
from nightly chron jobs.

5
00:00:23,215 --> 00:00:25,415
They were put into an Oracle database but

6
00:00:25,415 --> 00:00:30,005
that proved to be very a lot of
overhead and the data was very large.

7
00:00:30,005 --> 00:00:35,110
So they developed an ETL
using hang coded Python.

8
00:00:35,110 --> 00:00:41,190
And develop their own language called HQL,
which was a variant of SQL,

9
00:00:41,190 --> 00:00:46,770
and what HQL does is a query language for
Hive.

10
00:00:46,770 --> 00:00:51,420
It avoids some of the problems
that you get into with event,

11
00:00:51,420 --> 00:00:55,614
if you have an eventually
consistent sort of system.

12
00:00:55,614 --> 00:01:00,374
And you're looking at all these
things in a distributed maner from

13
00:01:00,374 --> 00:01:01,907
multiple machines.

14
00:01:01,907 --> 00:01:06,708
And HQL provides you a way to do that and
doesn't require

15
00:01:06,708 --> 00:01:12,170
you to have a rigid sort of
SQL type of interpretation.

16
00:01:12,170 --> 00:01:15,000
So what we're looking at with Hive is

17
00:01:15,000 --> 00:01:18,470
a translation of queries
into map reduced jobs.

18
00:01:18,470 --> 00:01:22,670
And this can run on top of Hadoop YARN.

19
00:01:22,670 --> 00:01:27,860
It runs on top of Tez or Spark as well,
which gives you alternate ways to

20
00:01:27,860 --> 00:01:32,420
actually implement it, and
sometimes those can be faster.

21
00:01:32,420 --> 00:01:36,940
Again, note here that
the database itself is

22
00:01:36,940 --> 00:01:42,300
not updated or changed in any
form while this is all happening.

23
00:01:42,300 --> 00:01:45,270
You're assuming a read-only database, and

24
00:01:45,270 --> 00:01:49,480
it's going to focus primary on
query part of the language SQL.

25
00:01:50,890 --> 00:01:57,320
The implementations, so
there are missing pieces of SQL if

26
00:01:57,320 --> 00:02:03,020
you like in HQL, but
they allow it to perform very fast.

27
00:02:03,020 --> 00:02:07,520
So, Hive looks very similar
to an SQL database.

28
00:02:07,520 --> 00:02:10,120
You can do a relational
join on two tables.

29
00:02:10,120 --> 00:02:16,180
If you're on the table of word counts,
Shakespeare's collection of words in

30
00:02:16,180 --> 00:02:21,930
a book, and you found a table of word
counts from a Homer on one of his books.

31
00:02:21,930 --> 00:02:24,790
And you want to actually compare them,
you can do so.

32
00:02:24,790 --> 00:02:28,990
And that would look like
an algorithm like this.

33
00:02:28,990 --> 00:02:32,599
What you're doing to
select from Shakespeare's.

34
00:02:35,199 --> 00:02:40,879
In here, and
you're going to get the frequency

35
00:02:40,879 --> 00:02:45,140
of the words that you had selected.

36
00:02:47,240 --> 00:02:51,500
And then you're going to join that
information with the same thing

37
00:02:51,500 --> 00:02:56,041
applied to another
select applied to Homer.

38
00:02:56,041 --> 00:03:01,520
So here's the Homer data with words.

39
00:03:01,520 --> 00:03:06,324
And what you would like to do is just to
look at the ones that have some sort of

40
00:03:06,324 --> 00:03:07,497
values in there.

41
00:03:07,497 --> 00:03:11,455
So we're asking the frequency
counts are bigger than zero, but

42
00:03:11,455 --> 00:03:14,173
in the Shakespeare and the Homer database.

43
00:03:14,173 --> 00:03:21,209
And then we're going to order
the whole result of that select so

44
00:03:21,209 --> 00:03:27,732
that what we have is a limit on
what we're going to print out.

45
00:03:27,732 --> 00:03:32,940
And it's going to be ten and we're going
to be doing it in descending order.

46
00:03:32,940 --> 00:03:36,350
And that's what's actually listed below.

47
00:03:37,560 --> 00:03:45,710
The ten top frequency words that appear
in the join between Shakespeare and

48
00:03:45,710 --> 00:03:50,630
Homer, and surprise, surprise the leads
the and it lead the pack there.

49
00:03:52,480 --> 00:03:56,060
So, behind the scenes,
what is that example actually doing?

50
00:03:57,330 --> 00:04:05,210
Well, what it does is to take the data
from these two collections, and

51
00:04:05,210 --> 00:04:10,480
it creates an abstract syntax tree, which
is sort of just basically a set tubules.

52
00:04:12,240 --> 00:04:16,460
And the words from those and
as you can see,

53
00:04:16,460 --> 00:04:21,840
what it's collected here
is where those words occur.

54
00:04:22,930 --> 00:04:29,250
It looks like a syntax tree where
you're taking the Shakespeare words,

55
00:04:29,250 --> 00:04:34,690
you're taking the whole words and
then you're applying different operators

56
00:04:34,690 --> 00:04:38,420
that are high primitives to those.

57
00:04:39,570 --> 00:04:44,690
And, as you extrude you will
see that each of those Hive,

58
00:04:44,690 --> 00:04:49,910
the HQL has been translated into this,
more sort of primitive sort of

59
00:04:50,980 --> 00:04:57,407
language of looking at each individual
item and applying a function to it.

60
00:04:57,407 --> 00:05:02,340
And what then occurs is
you take all of these and

61
00:05:02,340 --> 00:05:04,978
you map them into what I'm all,
MapReduce jobs.

62
00:05:04,978 --> 00:05:09,780
So for example, with the word, looking for

63
00:05:09,780 --> 00:05:13,470
how many, or counting the number of words,
what you're going to do is be building

64
00:05:13,470 --> 00:05:17,490
a MapReduce job that counts the number
of words from the Shakespeare job,

65
00:05:17,490 --> 00:05:22,140
and then from the Homer job, and then
you're going to go down here with a JOIN.

66
00:05:22,140 --> 00:05:25,343
You're going to write a similar MapReduce
job that maps these two things together.

67
00:05:25,343 --> 00:05:31,072
So Hive as you've seen is
actually got a shell and

68
00:05:31,072 --> 00:05:37,381
you can write pretty much
that language into a shell.

69
00:05:37,381 --> 00:05:41,601
And the introductory

70
00:05:41,601 --> 00:05:46,760
tutorials from Hive on our

71
00:05:46,760 --> 00:05:52,387
Horton works, looks well,

72
00:05:52,387 --> 00:05:57,311
builds that type of query

73
00:05:57,311 --> 00:06:02,020
in the shell like that.

74
00:06:02,020 --> 00:06:04,800
There's drivers that handle the sessions,
fetch and

75
00:06:04,800 --> 00:06:09,230
execute there's a compiler that does
the parsing and the planning and

76
00:06:09,230 --> 00:06:15,070
optimizing how the map reducers will
be performed and what analysis goes on.

77
00:06:15,070 --> 00:06:19,410
And then, there's an execution
engine that takes all of this

78
00:06:19,410 --> 00:06:23,070
description that sort of
abstract syntax tree and

79
00:06:23,070 --> 00:06:28,410
executes it as different
MapReduced jobs with HDFS and

80
00:06:28,410 --> 00:06:34,280
all the metadata to build the results,
which it then stores back in HDFS.

81
00:06:36,200 --> 00:06:39,020
Hive uses a traditional
database to store its metadata.

82
00:06:39,020 --> 00:06:43,350
It's a sort of namespace
containing a set of tables.

83
00:06:43,350 --> 00:06:47,970
Those table definitions contain
column types, physical layout,

84
00:06:47,970 --> 00:06:49,230
holds partitioning data.

85
00:06:49,230 --> 00:06:54,230
It could actually be used to store data in

86
00:06:54,230 --> 00:06:58,900
a variety of different, databases,
depending upon what you want.

87
00:06:58,900 --> 00:07:02,372
But if you can map them
into something like Hspace,

88
00:07:02,372 --> 00:07:05,295
you can get a much faster implementation.

89
00:07:08,767 --> 00:07:16,369
The Hive has a warehouse directory
in HDFS, as to built for Hive.

90
00:07:18,490 --> 00:07:21,280
And provides,

91
00:07:21,280 --> 00:07:27,400
there's a tutorial that provides lots of
examples on the warehouse in Hadoop works.

92
00:07:27,400 --> 00:07:31,160
The tables are stored in sub
directories of warehouses of warehouse.

93
00:07:31,160 --> 00:07:34,165
The partitions from
subdirectory of the tables

94
00:07:38,252 --> 00:07:46,030
Allow you to access pieces of each of
those tables on different machines.

95
00:07:46,030 --> 00:07:49,455
And then you're going to use
MapReduce over the whole thing

96
00:07:49,455 --> 00:07:51,125
to actually aggregate it.

97
00:07:51,125 --> 00:07:54,375
The actual data is stored in flat files.

98
00:07:54,375 --> 00:07:57,075
They are delimited text or
sequential files.

99
00:07:57,075 --> 00:08:03,342
And they can be customized to use any sort
of format, but mostly just text files.

100
00:08:03,342 --> 00:08:08,275
So this whole thing sort
of ends up looking like

101
00:08:08,275 --> 00:08:12,456
an HDFS with blocks holding ASCI text.

102
00:08:12,456 --> 00:08:18,399
That's actually input to do the
manipulations by your map reduce programs.

103
00:08:18,399 --> 00:08:28,399
[MUSIC]


